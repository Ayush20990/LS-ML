{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "physical_devices = tf.config.list_physical_devices('CPU')\n",
    "print(\"Available GPUs:\", physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 269 files belonging to 2 classes.\n",
      "Using 243 files for training.\n",
      "Class names: ['Bart', 'Homer']\n",
      "['Bart', 'Homer']\n"
     ]
    }
   ],
   "source": [
    "ds_train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    r'C:\\Users\\miral\\Assigment-1\\LS-ML-week1\\images', \n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    image_size=(64, 64),\n",
    "    shuffle=True,\n",
    "    seed=12,\n",
    "    validation_split=0.1,\n",
    "    subset='training'\n",
    ")\n",
    "class_names = ds_train.class_names\n",
    "print(\"Class names:\", class_names)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 269 files belonging to 2 classes.\n",
      "Using 26 files for validation.\n"
     ]
    }
   ],
   "source": [
    "ds_validation = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    r'C:\\Users\\miral\\Assigment-1\\LS-ML-week1\\images', \n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    batch_size=32,\n",
    "    image_size=(64, 64),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.1,\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Flatten(input_shape=(64,64,3)),\n",
    "    Dense(264, activation='relu',kernel_regularizer='l2'),\n",
    "    Dense(128, activation='relu',kernel_regularizer='l2'),\n",
    "    Dense(64,activation='relu',kernel_regularizer='l2'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 126ms/step - accuracy: 0.5390 - loss: 10.1525 - val_accuracy: 0.3462 - val_loss: 7.7784\n",
      "Epoch 2/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.5303 - loss: 6.9809 - val_accuracy: 0.7692 - val_loss: 5.8139\n",
      "Epoch 3/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.6954 - loss: 5.6868 - val_accuracy: 0.7308 - val_loss: 5.0482\n",
      "Epoch 4/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.7833 - loss: 4.9392 - val_accuracy: 0.3462 - val_loss: 5.1743\n",
      "Epoch 5/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.6016 - loss: 4.7162 - val_accuracy: 0.7308 - val_loss: 4.0775\n",
      "Epoch 6/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.7223 - loss: 4.0428 - val_accuracy: 0.8846 - val_loss: 3.5665\n",
      "Epoch 7/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8273 - loss: 3.5459 - val_accuracy: 0.9615 - val_loss: 3.2022\n",
      "Epoch 8/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8378 - loss: 3.2329 - val_accuracy: 0.8846 - val_loss: 2.9718\n",
      "Epoch 9/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.7664 - loss: 3.0756 - val_accuracy: 0.9615 - val_loss: 2.6797\n",
      "Epoch 10/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8077 - loss: 2.7866 - val_accuracy: 0.8462 - val_loss: 2.5453\n",
      "Epoch 11/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.7903 - loss: 2.6349 - val_accuracy: 0.8846 - val_loss: 2.3606\n",
      "Epoch 12/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.7852 - loss: 2.4485 - val_accuracy: 0.8846 - val_loss: 2.2539\n",
      "Epoch 13/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.7644 - loss: 2.3439 - val_accuracy: 0.9231 - val_loss: 2.0564\n",
      "Epoch 14/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.8133 - loss: 2.1379 - val_accuracy: 0.9615 - val_loss: 1.9043\n",
      "Epoch 15/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.9367 - loss: 1.9099 - val_accuracy: 0.8462 - val_loss: 1.9489\n",
      "Epoch 16/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.8934 - loss: 1.8612 - val_accuracy: 0.9615 - val_loss: 1.7075\n",
      "Epoch 17/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.9366 - loss: 1.7245 - val_accuracy: 0.7308 - val_loss: 1.8840\n",
      "Epoch 18/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.8286 - loss: 1.7357 - val_accuracy: 0.9615 - val_loss: 1.5845\n",
      "Epoch 19/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.8899 - loss: 1.5858 - val_accuracy: 0.8462 - val_loss: 1.6005\n",
      "Epoch 20/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8037 - loss: 1.6224 - val_accuracy: 0.9615 - val_loss: 1.4090\n",
      "Epoch 21/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.8744 - loss: 1.5133 - val_accuracy: 0.9615 - val_loss: 1.3650\n",
      "Epoch 22/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.9799 - loss: 1.3316 - val_accuracy: 0.9615 - val_loss: 1.3322\n",
      "Epoch 23/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 0.9263 - loss: 1.3379 - val_accuracy: 0.9615 - val_loss: 1.2796\n",
      "Epoch 24/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.9730 - loss: 1.2573 - val_accuracy: 0.9615 - val_loss: 1.2836\n",
      "Epoch 25/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 0.9405 - loss: 1.2297 - val_accuracy: 0.9615 - val_loss: 1.2136\n",
      "Epoch 26/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 0.9665 - loss: 1.1690 - val_accuracy: 0.8846 - val_loss: 1.2724\n",
      "Epoch 27/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.8957 - loss: 1.2055 - val_accuracy: 0.9231 - val_loss: 1.1854\n",
      "Epoch 28/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8982 - loss: 1.1999 - val_accuracy: 0.9615 - val_loss: 1.1629\n",
      "Epoch 29/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 0.9678 - loss: 1.0865 - val_accuracy: 0.9231 - val_loss: 1.1110\n",
      "Epoch 30/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.9621 - loss: 1.0588 - val_accuracy: 0.9231 - val_loss: 1.0828\n",
      "Epoch 31/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.9718 - loss: 1.0396 - val_accuracy: 0.9615 - val_loss: 1.0728\n",
      "Epoch 32/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 0.9640 - val_accuracy: 0.9231 - val_loss: 1.0612\n",
      "Epoch 33/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 0.9616 - loss: 0.9850 - val_accuracy: 0.9615 - val_loss: 1.0585\n",
      "Epoch 34/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - accuracy: 0.9678 - loss: 0.9673 - val_accuracy: 0.8462 - val_loss: 1.2186\n",
      "Epoch 35/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.9050 - loss: 1.0176 - val_accuracy: 0.9615 - val_loss: 0.9783\n",
      "Epoch 36/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.9827 - loss: 0.8827 - val_accuracy: 0.9615 - val_loss: 0.9476\n",
      "Epoch 37/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.9040 - loss: 1.0638 - val_accuracy: 0.4231 - val_loss: 1.8914\n",
      "Epoch 38/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.6739 - loss: 1.5954 - val_accuracy: 0.9231 - val_loss: 1.1178\n",
      "Epoch 39/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 0.7492 - loss: 1.2706 - val_accuracy: 0.8846 - val_loss: 1.1620\n",
      "Epoch 40/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.8717 - loss: 1.1146 - val_accuracy: 0.9231 - val_loss: 1.0713\n",
      "Epoch 41/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.9259 - loss: 1.0412 - val_accuracy: 0.9615 - val_loss: 0.9782\n",
      "Epoch 41: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17b4ef33050>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               mode='min',          \n",
    "                               patience=5,           \n",
    "                               verbose=1)        \n",
    "\n",
    "\n",
    "def normalize_img(image, label):\n",
    "   \n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "\n",
    "ds_train_normalized = ds_train.map(normalize_img)\n",
    "ds_validation_normalized = ds_validation.map(normalize_img)\n",
    "\n",
    "model.fit(ds_train_normalized,\n",
    "          epochs=100,\n",
    "          validation_data=ds_validation_normalized,\n",
    "          callbacks=[early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.15%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation dataset\n",
    "test_loss, test_accuracy = model.evaluate(ds_validation, verbose=0)\n",
    "\n",
    "# Convert accuracy to percentage format\n",
    "accuracy_percentage = test_accuracy * 100\n",
    "\n",
    "# Print accuracy in percentage format\n",
    "print(f\"Accuracy: {accuracy_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "Predicted class: 0\n",
      "Probability: 0.04\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(64, 64))  # Load and resize image\n",
    "    img_array = image.img_to_array(img)  # Convert to array\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Expand dimensions to match the input shape\n",
    "    img_array = img_array / 255.0  # Normalize pixel values\n",
    "    return img_array\n",
    "\n",
    "def predict_image(img_path, model):\n",
    "    img_array = load_and_preprocess_image(img_path)  # Load and preprocess image\n",
    "    prediction = model.predict(img_array)  # Make prediction\n",
    "    predicted_prob = prediction[0][0]  # Get the probability of the positive class\n",
    "    predicted_class = int(predicted_prob > 0.5)  # Classify as 1 if probability > 0.5, else 0\n",
    "    return predicted_class, predicted_prob\n",
    "\n",
    "# Assuming 'model' is your trained model\n",
    "# model = tf.keras.models.load_model('path_to_your_model.h5')\n",
    "\n",
    "img_path = \"images/Bart/bart3.bmp\"  # Replace with the path to your test image\n",
    "predicted_class, predicted_prob = predict_image(img_path, model)\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Probability: {predicted_prob:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted class: 1\n",
      "Probability: 0.98\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(64, 64))  # Load and resize image\n",
    "    img_array = image.img_to_array(img)  # Convert to array\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Expand dimensions to match the input shape\n",
    "    img_array = img_array / 255.0  # Normalize pixel values\n",
    "    return img_array\n",
    "\n",
    "def predict_image(img_path, model):\n",
    "    img_array = load_and_preprocess_image(img_path)  # Load and preprocess image\n",
    "    prediction = model.predict(img_array)  # Make prediction\n",
    "    predicted_prob = prediction[0][0]  # Get the probability of the positive class\n",
    "    predicted_class = int(predicted_prob > 0.5)  # Classify as 1 if probability > 0.5, else 0\n",
    "    return predicted_class, predicted_prob\n",
    "\n",
    "# Assuming 'model' is your trained model\n",
    "# model = tf.keras.models.load_model('path_to_your_model.h5')\n",
    "\n",
    "img_path = \"images/Homer/homer7.bmp\"  # Replace with the path to your test image\n",
    "predicted_class, predicted_prob = predict_image(img_path, model)\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Probability: {predicted_prob:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
